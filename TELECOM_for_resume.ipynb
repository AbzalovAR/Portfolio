<a id='nachalo'></a>

<div style="border-radius: 40px; box-shadow: 4px 4px 4px; border: solid blue 10px; padding: 10px">


<h1 align="right"> ПРОЕКТ ТЕЛЕКОМ </h1>

<font color='crimson'>

<h1 align="center"> ОПРЕДЕЛЕНИЕ НЕЭФФЕКТИВНЫХ ОПЕРАТОРОВ </h1>
</font>
<h3 align="right"> Выполнил: Абзалов Артур</h3>
<h3 align="right"> Телефон: +7(932)422-34-33</h3>

<h3 align="right"> 23.09.2023</h3>

Ссылка на презентацию: https://yadi.sk/i/kXAu71bVo08xvA

Ссылка на дашборд: https://public.tableau.com/app/profile/artur6529/viz/Telecom_1_16964128666420/Dashboard1

<a id='0'></a>
<font color='crimson'>  
<h2 align="center"> Описание проекта</h2>
</font>

Наш заказчик - Телеком провайдер «Нупозвони». Клиенты заказчика − колл-центры, которые:
* распределяют входящие вызовы на операторов,
* совершают исходящие вызовы силами операторов.<br>
Также операторы могут делать внутренние вызовы — вызовы друг между другом внутри сети виртуальной телефонии. <br>
Наша задача разработать методику, по которой можно будет определять неэффективных
операторов автоматически, чтобы заказчик смог предлогать своим клиентам новый сервис для регулярного мониторинга «Нупозвони мне, нупозвони!»

**Задача:**
**разработать методику, по которой можно будет определять неэффективных
операторов автоматически.** <br>
Признаки низкой эффективности:
* много пропущенных входящих вызовов,
* долгое ожидания ответа при входящих внешних вызовах,
* мало исходящих вывозов — в случае тех колл-центров, что специализируются на обзвонах, а не обработке входящих звонков.

<a id='0'></a>
<font color='crimson'>
    <h2 align="center"> Содержание</h2>
</font>

0. [Описание проекта](#1)<br>
1. [Содержание](#0)<br>
2. [Обзор данных](#2)<br>
3. [Подготовка данных](#3)<br>
    3.1. [Объединим датафреймы](#3-1)<br>
    3.2. [Удалим дубликаты](#3-2)<br>
    3.3. [Обработаем пропуски](#3-3)<br>
    &ensp; 3.3.1. [пропуски в столбце `operator_id`](#3-3-1)<br>
    &ensp; 3.3.2. [пропуски в столбце `internal`](#3-3-2)<br>
    &ensp; 3.3.3. [приведем в соответствие данные столбца `is_missed_call`](#3-3-3)<br>
    3.4. [Преобразуем типы данных](#3-4)<br>
    3.5. [Найдем и уберем отклонения(выбросы)](#3-5)<br>
    3.6. [Проверим категориальные столбцы](#3-6)<br>
    3.7. [Добавим дополнительный столбец](#3-7)<br>
    3.8. [Отобразим корреляцию](#3-8)<br>
    3.9. [Создадим дополнительные столбцы](#3-9)<br>

4. [Исследоваиельский анализ](#4)<br>
    4.1. [Разделим колл-центры по специализации](#4-1)<br>
    4.2. [Проверим связь специализации с тарифным планом](#4-2)<br>
    4.3. [Оперелим границы эффективности операторов в колл-центрах,<br>
которые **специализируются на обработке входящих звонков**](#4-3)<br>
    &ensp; 4.3.1. [по пропущенным внешним вызовам](#4-3-1)<br>
    &ensp; 4.3.2. [по длительности ожидания ответа](#4-3-2)<br>
    &ensp; 4.3.3. [по длительности самого ответа](#4-3-3)<br>
    4.4. [Опереление границ эффективности операторов в колл-центрах,<br>
которые **специализируются на обзвонах**](#4-4)<br>
    &ensp; 4.4.1. [по количеству исходящих внешних звонков](#4-4-1)<br>
    &ensp; 4.4.2. [по продолжительности самого звонка](#4-4-2)<br>

5. [Определим неэффективных операторов](#5)<br>
    5.1. [Определим неэффективных операторов в колл-центрах, которые **специализируются на обработке входящих звонков**](#5-1)<br>
    &ensp; 5.1.1. [по пропущенным внешним вызовам](#5-1-1)<br>
    &ensp; 5.1.2. [по длительности ожидания ответа](#5-1-2)<br>
    &ensp; 5.1.3. [по длительности ответа](#5-1-3)<br>
    5.2. [Определим неэффективных операторов в колл-центрах, которые **специализируются на обзвонах**](#5-2)<br>
    &ensp; 5.2.1. [по количеству исходящих внешних звонков](#5-2-1)<br>
    &ensp; 5.2.2. [по продолжительности звонка](#5-2-2)<br>
 
6. [Проверим гипотезы](#5)<br>
    6.1. [Эффективность работы оперторов в выходные дни и в будни отличается](#6-1)<br>
    6.2. [В выходные дни и в будни относительное количество пропущенных звонков одинаковое](#6-2)<br>
    
7. [Выводы](#itog)

<a id='2'></a>
<font color='crimson'>
<h2 align="center"> Обзор данных</h2>
</font>

Импортируем необходимые библиотеки

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from datetime import datetime, timedelta,date

#from scipy import stats as st
import scipy

 Расширим ячейки для удобства

# Расширим ячейки до 90% ширины
from IPython.core.display import display, HTML 
display(HTML("<style>.container { width:80% !important; }</style>")) # Можно выставить и другую ширину изменив проценты<a id='1'></a>

Загрузим датасеты:

# Загрузим первый датасет
try:
    telecom_dataset = pd.read_csv('telecom_dataset.csv', parse_dates=['date'],infer_datetime_format=True) 
except:
    telecom_dataset = pd.read_csv('https://code.s3.yandex.net/datasets/telecom_dataset.csv', parse_dates=['date'],infer_datetime_format=True)

# Загрузим второй датасет    
try:
    telecom_clients = pd.read_csv('telecom_clients.csv', parse_dates=['date_start'],infer_datetime_format=True)
except:
    telecom_clients = pd.read_csv('https://code.s3.yandex.net/datasets/telecom_clients.csv', parse_dates=['date_start'],infer_datetime_format=True)    

Напишем функцию для разведывательного анализа датафреймов

def analiz_nachalo(data):
    """
        Эта функция для первичного анализа данных.
        
    """    
    print('\b'+'\033[1m'+'\033[91m'+' Датафрейм содержит \n столбцов ', data.shape[1], 
          ' шт. и строк ', data.shape[0], ' шт.'+'\033[0m \n') 

    print('\b'+'\033[1m'+'\033[91m'+' Первые 5 строк:'+'\033[0m')
    pd.set_option('display.max_columns', None) # чтобы отобразить все столбцы
    display(data.head())
    
    print('\033[1m'+'\033[91m'+' Последние 5 строк:'+'\033[0m')
    pd.set_option('display.max_columns', None) # чтобы отобразить все столбцы
    display(data.tail())
    
    
    print('\n\b'+'\033[1m'+'\033[91m'+' Выведем общую информацию о датафрейме:'+'\033[0m \n')        
    data.info(show_counts=True) # show_counts=True - отобразить количество заполненых колонок
    
    # Проверим есть ли пропуски
    n = 0
    for i in data.columns:
        n += data[i].isna().sum() 
    if n > 0:
        print('\n\b'+'\033[1m'+'\033[91m'+' Как видим в датафрейме имеются пропуски в следующих столбцах:'+'\033[0m')
        for i in data:
            if data[i].isna().sum() > 0:
                print('    в столбце '+'\033[94m'+f'{i}'+'\033[0m'+' = ', data[i].isna().sum(), 'шт.'+'\033[0m')
        print('    в остальных столбцах пропусков нет')

        #print ('\033[1m'+'\033[91m'+'\n Отобразим пропуски визуально'+'\033[0m')
        #plt.figure(figsize=(15,10))
        #sns.heatmap(data.isna(), yticklabels=False, cbar=False, cmap="YlGnBu") # не забудь импортнуть библиотеку seaborn  import seaborn as sns
        #plt.show()
    else: print('\n\b'+'\033[1m'+'\033[91m'+' Пропусков в данных нет - прекрасно!'+'\033[0m')
        
    print('\n\b'+'\033[1m'+'\033[91m'+' Количество явных дубликатов:'+'\033[0m', data.duplicated().sum())  
    
    try:
        print(' \n\b'+'\033[1m'+'\033[91m'+' Выведем информацию о числовых столбцах\n (показатели описательной статистики):'+'\033[0m')
        display(data.describe()) #  вычисление показателей описательной статистики

        print('\b'+'\033[1m'+'\033[91m'+' Построим гистограммы для всех числовых столбцов датасета:'+'\033[0m')          
        data.hist(figsize=(15,5), bins=50); 
        plt.suptitle('Гистограммы распределения признаков') # Должна быть импортированна библиотека матплотлиб  import matplotlib.pyplot as plt
        plt.show()
    except: print('    гистограммы не возможно вывести')
            
    print('\n\b'+'\033[1m'+'\033[91m'+' Проверим категориальные столбцы:\n'+
          '(за категориальные столбцы программа принимает столбцы, \n'+
          'в которых уникальных значений меньше "50") \033[0m')  
    for i in data:
        if data[i].nunique() < 50: # Если уникальных значений столбца больше условно 50 - будем считать его числовым, 
                                # в некоторых случаях 50 нужно уменьшить/увеличить
            print('    Уникальные значения столбца'+'\033[94m'+f' {i}:'+'\033[0m'+'')
            print(data[i].sort_values().unique())
            print('    Статистика распределения уникальных значений:')
            print(data[i].sort_values().value_counts(normalize=True), '% \n')
    
    print('\n \033[94m \033[01m'+'*'*99)

datetime_is_numeric=True

Посмотри оба датафрейма:

**Датасет `telecom_dataset`:**<br>
`user_id` — идентификатор клиента;<br>
`date` — дата статистики;<br>
`direction` — направление вызовов: `out` — исходящий вызов, `in` — входящий;<br>
`internal` — маркер внутренних (**True**) и внешних (**False**) вызовов;<br>
`operator_id` — идентификатор оператора;<br>
`is_missed_call` — маркер пропущенных вызовов;<br>
`calls_count` — количество звонков;<br>
`call_duration` — длительность звонка без учёта времени ожидания (**в секундах**);<br>
`total_call_duration` — длительность звонка с учётом времени ожидания (**в секундах**).<br>

analiz_nachalo(telecom_dataset)

<div style="border-radius: 22px; box-shadow: 4px 4px 4px; border: solid green 2px; padding: 16px">


**Выводы**
* датасет содержит 9 столбцов и 53902 записи о звонках,
* стиль названий столбцов корректен,
* имеются явные дубликаты - удалим,
* треюуется исправить типы столбцов:<br>
 тип столбца `date` переделаем на datetime,<br>
 тип столбца `internal` переделаем в bool,<br>
 тип столбца `operator_id` - привести к типу int,
* требуется обработать пропуски в столбцах `operator_id` и `internal`.

**Датасет `telecom_clients`:**<br>
`user_id` — идентификатор клиента;<br>
`tariff_plan` — тарифный план клиента;<br>
`date_start` — дата регистрации клиента в сервисе.<br>

analiz_nachalo(telecom_clients)

Псомотрим у всех ли датасетов период данных совпадает

def time_min_max(data):
    print('столбец', data.name)
    print('минимальное время', data.min())
    print('максимальное время', data.max())
    print();

time_min_max(telecom_dataset['date'])
time_min_max(telecom_clients['date_start'])

Период времени не совпадает. <br>Скоре всего потому, что в столбце `date_start` дата регистрации клиента, а в столбце `date` - даты взаимодействия с клиентом.

print('Уникальных клиентов в telecom_dataset: ', telecom_dataset['user_id'].nunique())
print('Уникальных клиентов в telecom_clients: ', telecom_clients['user_id'].nunique())


Из 732 клиентов улугами телеком провайдера воспользовались только 307.

<div style="border-radius: 22px; box-shadow: 4px 4px 4px; border: solid green 2px; padding: 16px">


**Выводы:**
* названия столбцов корректны,
* тип столбца `date_start` привести к datetime,
* пропусков нет,
* дубликатов нет, 
* периоды времени адекватны,
* из 732 зарегестрированных клиента, взаимодействие происходило только с 307 клиентами.

<a id='3'></a>
<font color='crimson'>

<h2 align="center"> Подготовка данных</h2>
</font>

<a id='3-1'></a>

<font color='crimson'>

### Объединим датафреймы
</font>

[К началу раздела](#3)<br>
[К соддержанию](#0)

Чтобы определить стаж работы операторов, а так же проверить взаимосвязь с тарифными планами нам следует объединить датафреймы:

df = telecom_dataset.merge(telecom_clients, on='user_id', how='left')
df.head()

df.info()

<div style="border-radius: 22px; box-shadow: 4px 4px 4px; border: solid green 2px; padding: 16px">

Датафреймы объединили

<a id='3-2'></a>

<font color='crimson'>

### Удалим дубликаты
</font>

[К началу раздела](#3)<br>
[К соддержанию](#0)

Удалим явные дубликаты:

print('\033[1m'+'Явных дубликатов до:'+'\033[22m', df.duplicated().sum())
df = df.drop_duplicates()
print('\033[1m'+'Явных дубликатов после:'+'\033[22m', df.duplicated().sum())

Проверим на неявные дубликаты:
Так как данные в датасете агрегированные, здесь так же не должно быть дубликатов по одинаковым критериям: <br>
**user_id + дата + operator_id + direction + internal + is_missed_call**.

# проверяем неявные дубликаты:
df[['user_id', 'date', 'direction', 'internal', 'operator_id',
       'is_missed_call']].duplicated().sum()

Не явных дубликатов нет.

Теперь проверим нет ли дубликатов среди **user_id** клиентов в таблице `telecom_clients`:

telecom_clients['user_id'].duplicated().sum()

<div style="border-radius: 22px; box-shadow: 4px 4px 4px; border: solid green 2px; padding: 16px">

**Выводы:**<br> 
В датасетах удалены янвые дубликаты.<br> 
Произведена проверка на неявные дубликаты - неявные дубликаты не обнаружены. <br> 

<a id='3-3'></a>

<font color='crimson'>

### Обработаем пропуски
</font>

[К началу раздела](#3)<br>
[К соддержанию](#0)

<a id='3-3-1'></a>

<font color='crimson'>

#### Пропуски в столбце <font color='blue'> operator_id</font>

</font>

Попробуем разобраться в причинах возникновения пропусков в столбце `operator_id`.
Посмотрим на некоторые строки с пропусками:

df_nan = df.loc[df['operator_id'].isna()]
df_nan.head(10)

Сразу в глаза бросаются одинаковый алгоритм возникновения NaN - все вызовы входящие, внешние и пропущенные. 
Проверим количество уникальных значений в столбцах `direction`, `internal` и `is_missed_call`:

display('Количество строк относительно уникальных значений в столбце direction:', 
        df_nan.groupby('direction')[['user_id', 'internal', 'is_missed_call']].count())
display('Количество строк относительно уникальных значений в столбце internal:', 
        df_nan.groupby('internal')[['user_id', 'direction', 'is_missed_call']].count())
display('Количество строк относительно уникальных значений в столбце is_missed_call:', 
        df_nan.groupby('is_missed_call')[['user_id', 'direction', 'internal']].count())

Как видим, основная масса - это пропущенные внешние входящие звонки. 
Будем считать, что эти пропуски возникли из-за того, что программа не успела назначить оператора, либо звонок происходил в нерабочее время. <br>
Но есть не малое колчество других вариаций возникновения пропусков в столбце операторов.

Так как мы исследуем работу операторов, а за пропусками в разных колл_центрах могут скрываться либо один оператор либо десять,<br>
пропуски в столбце `operator_id` являются критическими. 
Так же, чтобы случайно не ошибиться при фильтрации оперпторов с наличием ID и без,<br>
**далее будем анализировать датасет без пропусков  **


Создадим копию основного датасета c пропусками операторов, для сохранения исходных данных, 
но продолжим работу с очищенными данными:

# Создадим копию основного датасета, чтобы не потерять данные
data_reserv = df.copy()

# Удалим пропуски в столбце operator_id 
df = df.loc[df['operator_id'].isna() == False]

# Сколько пропусков осталось
display(df[['operator_id','internal']].isna().sum())

Малая часть звонков имеют категории исходящих, внутренних, отвеченных.<br>
Исходящие без назначения оператора могли возникнуть при внутреннем звонке одного оператора другому (например, с целью консультации). В таком случае все исходящие должны быть внутренними. 
Но проверка этого момента не подтвердила данное предположение. Код:
        
        df.loc[(operator_id_nan['direction'] == 'out') & (df['internal'] == False)& (df['total_call_duration']>10)].head(15)
        
Другое предположение, о том что пропущенные значения в operator_id появлялись из-за, того что программа не успевала назначить оператора, так же не нашла подтверждения. Код:
        
        df.loc[df['operator_id'] < 2, 'avg_wait_time'].max()      
        df.loc[(df['operator_id'] < 2)& (df['avg_wait_time']>50)]        
Получилось слишком много случаев, когда времени было достаточно для нащначения оператора.


Поэтому, **вывод**: природа появления остальных пропусков в столбце `operator_id` точно не ясна, и относительно слишком мала, чтобы уделять разбору причин слишком много времени, поэтому пока заполним заглушками "1", и вернемся при необходимости.
В реальной работе, можно было бы обсудить с менеджментом о возможных причинах возникновения и заполнить пропуски иным способом.

<a id='3-3-2'></a>

<font color='crimson'>

#### Пропуски в столбце <font color='blue'> internal</font>

</font>

Поробуем выяснить причины возникновения пропусков в столбце `internal`. Взглянем на пропуски и униакльные значения:

df.loc[df['internal'].isna()].tail() # Но можно взглянуть и на все строки целиком их осталось 55 - не так уж и много

Общий алгоритм не прослеживается.

Посмотрим количество уникальных значений

df_nan= df.loc[df['internal'].isna()]
display('Количество строк уникальных значений столбца direction, где пропущены значения в internal: ', 
        df_nan.groupby('direction')[['user_id', 'is_missed_call']].count())
display('Количество строк уникальных значений столбца is_missed_call, где пропущены значения в internal:',
        df_nan.groupby('is_missed_call')[['user_id', 'direction']].count())

Все звонки входящие. И все, кроме одного не пропущенные. 

Удалим пропуски в столбце `internal`, так как невозможно достоверно определить внешний или внутрений был вызов, а так же потому, что данных пропущенных очень мало относительно всего датасета.

df = df.loc[df['internal'].isna() == False]
# Сколько пропусков осталось
#display(df.isna().sum())
#df.info()

<div style="border-radius: 22px; box-shadow: 4px 4px 4px; border: solid red 2px; padding: 16px">

**Более подробный просмотр данных, обнаружил, что есть строки, где общее время звонка больше времени ответа. 
В принципе, наличие времени в столбце "время звонка без учета ожидания", уже говорит говорит о том, что звонок был принят.
Но в столбце `is_missed_call` стоит значение True - якобы он пропущенный.
Это вынуждает выполнить проверку датасета на верность трактования столбца `is_missed_call`:**

<a id='3-3-3'></a>

<font color='crimson'>

#### приведем в соответствие данные столбца <font color='blue'> is_missed_call</font>

</font>

df.loc[(df['is_missed_call']==True)&(df['call_duration']>0)].head()

Строки имеют маркер "пропущеный" и при этом время ответа больше ноля. Это похоже на ошибки программы (какие либо сбои). По этому приведем в соответсвие:

# Будем считать овеченными звонки, которые длились больше 0 секунд
df.loc[(df['is_missed_call']==True)&(df['call_duration']>0), 'is_missed_call'] = False

<div style="border-radius: 22px; box-shadow: 4px 4px 4px; border: solid green 2px; padding: 16px">

**Выводы:**
* Пропуски в столбце `operator_id` удалены, так как для анализа эффективности работы оперторов данные об ID операторов имеет критическое значение.      
* Пропуски в столбце `internal` удалили, так как количество пропусков было мало, плюс, я не нашел логических объяснений их происхождения, и, как следствие, не понимаю чем их заполнять.
* По ходу анализа были обнаружены ошибки маркировки отвеченых звонков в столбце `is_missed_call` - строки приведены в соответсвие. <br> В реальной работе информацию об ошибках я бы передал менеджменту или разработчикам.

<a id='3-4'></a>

<font color='crimson'>

### Преобразуем типы данных
</font>

[К началу раздела](#3)<br>
[К соддержанию](#0)

#df['date'] = pd.to_datetime(df['date'],utc=False).dt.date
#df['date_start'] = pd.to_datetime(df['date_start'],utc=False).dt.date

df = df.astype({'internal':'bool', 'operator_id':'int'})
df.info()

<div style="border-radius: 22px; box-shadow: 4px 4px 4px; border: solid green 2px; padding: 16px">

**Выводы:**

Преобразовали типы данных в столбцах: 
 * `operator_id` в int 
 * `internal` в bool 

<a id='3-5'></a>

<font color='crimson'>

### Найдем и уберем отклонения(выбросы)
</font>

[К началу раздела](#3)<br>
[К соддержанию](#0)

Еще раз посмортим на показатели столбцов

df.describe()

Бросаются в глаза выбросы в столбце `calls_count`. Но и остальные стоит проверить.

Посмотрим выбросы с учетом межквартильного размаха.

Межквартильный размах (IQR) — это разница между 75-м процентилем (Q3) и 25-м процентилем (Q1) в наборе данных. Он измеряет разброс средних 50% значений.<br>
    
    Выбросы = наблюдения > Q3 + 1,5 IQR или Q1 – 1,5 IQR (или +/- 3 IQR для экстремальных выбросов) 

# Cчитаем выбросы с помощью межквартильного размаха
def anomalii(df):
    """
        Эта функция в каждом столбце датасета высчитывает межквартильные размахи
        После чего выводит на экран сообщение о наличии/отсутсвии выбросов в столбцах
        В данном случае закоментированы нижние границы выбросов, так как в них нет необходимости
        при желании можно их посмотреть
    """
    for i in df.columns:
      q1 = df[i].quantile(0.25)
      q3 = df[i].quantile(0.75)
      max = df[i].max()
      iqr = q3 - q1
      whisker_up_moderate = q3 + 1.5 * iqr
      whisker_down_moderate = q3 - 1.5 * iqr
      whisker_up_extreme = q3 + 3 * iqr
      #print(f'Максимальное значение в столбце'+'\033[94m'+f' {i}'+'\033[0m'+f': {max}')
      #print(f'Верхняя граница для умеренных выбросов, столбец {i}: {whisker_up_moderate}')
      #print(f'Нижняя граница для умеренных выбросов, столбец {i}: {whisker_down_moderate}\n')
      #print(f'Верхняя граница для экстремальных выбросов, столбец {i}: {whisker_up_extreme}')
      if max <= whisker_up_moderate:
            print('В столбце'+'\033[94m'+f' {i}'+'\033[0m'+' аномальных значений нет')
      else: 
        print('\033[91m'+'\033[1m'+'В столбце'+f' {i} возможно есть аномальные значения:'+'\033[0m')
        print(f'    Максимальное значение в столбце'+'\033[94m'+f' {i}'+'\033[0m'+f': {max}')
        print(f'    Верхняя граница для умеренных выбросов в столбце {i}: {whisker_up_moderate}')
        print(f'    Верхняя граница для экстремальных выбросов в столбце {i}: {whisker_up_extreme}')
        #print(f'    Нижняя граница для умеренных выбросов, столбец {i}: {whisker_down_moderate}')
      print()

anomalii(df[['date', 'operator_id','calls_count', 'call_duration','total_call_duration', 'date_start']])

Посмортим много ли выбросов по 99 процентилю в столбцах с аномальными значениями:

# Запустим цикл проверки выбросов в столбцах с подозрением на выбросы
for i in df[['calls_count', 'call_duration', 'total_call_duration']]:
        print(f'количество выбросов в столбце {i} =',df.loc[df[i] > (df[i].quantile(0.99)), i].count())

plt.figure(figsize=(20,1))
plt.title('Графики межквартильного размаха',  fontsize=16)
sns.boxplot(data=df, x='calls_count',medianprops={"color": "coral"})
plt.xlabel('Количество звонков', fontsize=12)
plt.show()

plt.figure(figsize=(20,1))
sns.boxplot(data=df, x='call_duration',medianprops={"color": "coral"})
plt.xlabel('Длительность звонков без учета ожидания', fontsize=12)
plt.show()

plt.figure(figsize=(20,1))
sns.boxplot(data=df, x='total_call_duration',medianprops={"color": "coral"})
plt.xlabel('Общая длительность звонков', fontsize=12)
plt.show()

Увеличим нижнюю часть "boxplot"-ов, чтобы лучше разглядеть медианы и 75% квантиль

plt.figure(figsize=(20,1))
plt.title('Графики межквартильного размаха',  fontsize=16)
sns.boxplot(data=df, x='calls_count', notch=True, medianprops={"color": "coral"})
plt.xlabel('Количество звонков', fontsize=12)
plt.xlim(40,0)
plt.show()

plt.figure(figsize=(20,1))
sns.boxplot(data=df, x='call_duration', notch=True, medianprops={"color": "coral"})
plt.xlabel('Длительность звонков без учета ожидания', fontsize=12)
plt.xlim(1500,-10)
plt.show()

plt.figure(figsize=(20,1))
sns.boxplot(data=df, x='total_call_duration', notch=True, medianprops={"color": "coral"})
plt.xlabel('Общая длительность звонков', fontsize=12)
plt.xlim(2500,-10)
plt.show()

Выбросов мало, можно удалить все.
Посмортим значения 99 процентиля:

display(df[['calls_count', 'call_duration', 'total_call_duration']].quantile([0.95, 0.99]))

Не всегда аномальные значения означают выбросы.<br>
Удаляя по 95 процентилю мы теряем большую часть данных, которые в принципе являются абослютно реальными значениями:<br>
Что такое 62 звонка в сутки? - Абсолютно реальное значение,<br>
4161 и 5035 секнуд длительности звонков - это 69 и 84 минуты длительности звонков - так же абсолютно реальные значения.

Тем не менее выбросы по 99 процентилю удалю, чтобы при анализе они не тянули на себя срдение значения:

a = (df['calls_count'].quantile(0.99)).astype(int)
b = (df['call_duration'].quantile(0.99)).astype(int)
c = (df['total_call_duration'].quantile(0.99)).astype(int)
print(a,b,c)

df = df.loc[(df['calls_count'] < a) & 
            (df['call_duration'] < b) & 
            (df['total_call_duration'] < c)]

Построим боксплоты для столбцов с подозрением на аномалии:

<div style="border-radius: 22px; box-shadow: 4px 4px 4px; border: solid green 2px; padding: 16px">

**Выводы:**

Аномалии удалии по 99 процентилю

<a id='3-6'></a>

<font color='crimson'>

### Проверим категориальные столбцы
</font>

[К началу раздела](#3)<br>
[К соддержанию](#0)

Так как в данных часто содержатся опечатки или неявные дубликаты посмотрим все униальные значения категориальных столбцов и, при необходимости, обработаем их.
Посмотрим уникальные значения категорияльных столбцов и их относительное распределение:

# Запустим цикл для просмотра уникальных значений в категориальных столбцах:
def see_kategori_columns(df):
    for i in df:
        if df[i].nunique() < 50: # Если уникальных значений столбца больше условно 50 - будем считать его числовым, 
                                # в некоторых случаях 50 нужно уменьшить/увеличить
            print('    Уникальные значения столбца'+'\033[94m'+f' {i}:'+'\033[0m'+'')
            print(df[i].sort_values().unique())
            print('    Статистика распределения данных значений:')
            print(df[i].value_counts(normalize=True),'\n')

see_kategori_columns(df)

<div style="border-radius: 22px; box-shadow: 4px 4px 4px; border: solid green 2px; padding: 16px">

**Выводы:**

* Исходящих вызовов значительно больше, чем входящих.
* В основном звонки внешние - это логично.
* Треть звонков - пропущенные.
* В данных представлены 3 тарифа - A, B и C. Наиболее популярный тариф С и В. У каждого колл-центра за исследуемый период был только один тариф - тарифы не менялись.<br> 

# Посмотрим, что осталось
temp = df.copy() 
list_c = ['user_id', 'date', 'direction', 'internal', 'operator_id',
       'is_missed_call', 'calls_count', 'call_duration', 'total_call_duration']
print(temp.info())
for col_l in list_c:
  print('-'* 25)
  print('\033[94m Столбец ', col_l,'\033[0m', temp[col_l].sort_values().unique())
  print('\033[94m',col_l,': кол-во NaN',temp[col_l].isna().sum(),
        ', процент NaN', round(temp[col_l].isna().mean()*100,2),'%'+'\033[0m') 

Сгруппируем данные по типам звонков и проверим каких звонков больше:

call_type_group = df.groupby(['direction','internal','is_missed_call'])['user_id'].agg('count')
call_type_group = call_type_group.reset_index()
call_type_group

Для восприятия и анализа будет удобнее, если для каждого вызова мы агрегируем его тип на русском языке, 
например: внешний, входящий, пропущеный:

call_type_group.columns = ['входящий/исходящий','внешний/внутрений','пропущенный/отвеченый','сумма_звонков']

def type_call(df):
    if df['входящий/исходящий'] == 'in':
        df['тип_звонка'] = 'входящий_'
    elif df['входящий/исходящий'] == 'out':
        df['тип_звонка'] = 'исходящий_'
    
    if df['внешний/внутрений'] == True:
        df['тип_звонка'] = df['тип_звонка'] + 'внутренний_'
    elif df['внешний/внутрений'] == False:
        df['тип_звонка'] = df['тип_звонка'] + 'внешний_'
        
    if df['пропущенный/отвеченый'] == True:
        df['тип_звонка'] = df['тип_звонка'] + 'пропущенный'
    elif df['пропущенный/отвеченый'] == False:
        df['тип_звонка'] = df['тип_звонка'] + 'непропущенный'
    return df

call_type_group = call_type_group.apply(type_call, axis=1)

call_type_group = call_type_group[['тип_звонка', 'сумма_звонков']]
call_type_group = call_type_group.sort_values(by='сумма_звонков', ascending=False)
# Было бы удобно оценить и доли
call_type_group['процент, %'] = round((call_type_group['сумма_звонков']/call_type_group['сумма_звонков'].sum()),3)*100
display(call_type_group)

Визуализируем данные:

plt.figure(figsize=(15,5))
sns.barplot(data=call_type_group, y='тип_звонка', x="сумма_звонков",palette=sns.color_palette("BuGn_r", n_colors=len(call_type_group)))
plt.title('Распеределение по типам звонков')
plt.xlabel('количество звонков')
plt.ylabel('тип звонка')
plt.show()

Наиболее часто совершались внешние исходящие и входящие звонки. Это понятно - колл-центры нацелены на работу с населением а не внутри компании.

<a id='3-8'></a>

<font color='crimson'>

### Отобразим корреляцию
</font>

[К началу раздела](#3)<br>
[К соддержанию](#0)

Рассчитаем матрицу корреляций и на её основе построим heatmap

Чтобы посмотреть корреляцию с тарифами и входящими/исходящими звонками, переведем буквенные обозначения в цифровые:

# Функция для замены одного значения на другое
def replacing_values(data, column, wrong_values, correct_values):
    data[column] = data[column].replace(wrong_values, correct_values)

# Создадим вспомогательные столбцы, чтобы не портить основные данные
df['tariff'] = df['tariff_plan']
df['direct'] = df['direction']

# Оцифруем тарифы
replacing_values(df,['tariff'], 'A', 1)
replacing_values(df,['tariff'], 'B', 2)
replacing_values(df,['tariff'], 'C', 3)

# Оцифруем Входящие/исходящие
replacing_values(df,['direct'], 'in', 1)
replacing_values(df,['direct'], 'out', 0)

# Проверим
print('Тарифы: ', df['tariff'].sort_values().unique())
print('Вх-ие/Исх-ие: ', df['direct'].sort_values().unique())

cm = df.corr()

#нарисуем тепловую карту с подписями для матрицы корреляций
plt.figure(figsize=(15,10))
sns.heatmap(cm, annot=True, linewidth=1, square=True, fmt='.2f')
plt.title('Матрица корреляций')
plt.show()

Для оценки тесноты, или силы, корреляционной связи мы будем использовать общепринятые критерии, согласно которым абсолютные значения корреляции < 0.3 свидетельствуют о слабой связи, <br>
значения корреляции от 0.3 до 0.7 - о связи средней тесноты, <br>
значения корреляции > 0.7 - о сильной связи.

Сильная корреляция между столбцами длительности звонков - понятна.
Так же понятна и средняя корреляция между количеством звонков `calls_count` и их длительностью `call_duration`, `total_call_duration` - чем больше звонков тем больше и их длительность.
Скорее всего этим объясняется и средняя корреляция между длительностью звонков и маркером пропущенности `is_missed_call`, хотя с количестовом звонков данный маркер не коррелирует ни как.
Средняя кореляция между id операторов и входящимим/исходящими - не совсем ясна.
Посмотрим попарные дигарамм рассеяния для этих признаков:

# построим попарные диаграммы рассеяния 
for i in ['direct', 'is_missed_call']:
    for col in ['operator_id','call_duration', 'total_call_duration']:
        plt.figure(figsize=(15,2))
        sns.scatterplot(x=df[col], y=df[i], alpha = 0.2)
        plt.show()

Анализ попарных дигарамм рассеяния дополнительных корреляций не обнаружил.
<br> Удалим вспомогательные столбцы:

df = df[['user_id', 'date', 'direction', 'internal', 'operator_id',
       'is_missed_call', 'calls_count', 'call_duration', 'total_call_duration',
       'tariff_plan', 'date_start']]
#df.head()

<a id='3-9'></a>

<font color='crimson'>

### Создадим дополнительные столбцы
</font>

[К началу раздела](#3)<br>
[К соддержанию](#0)

Создадим дополнительные столбцы:<br>
* общего времени ожидания вызова `wait_time`,
* среднего времени ожидания вызова `avg_waittm`,
* среднего времени разговора `avg_callt`,
* период работы оператора `lvt_oper_id`:

df['wait_time'] = df['total_call_duration'] - df['call_duration']
# среднего времени разговора avg_call_time
df['avg_callt'] = df['call_duration']/df['calls_count']
# среднего времени ообщей длительности звонка - avg_total_time:
# df['avg_total_time'] = df['total_call_duration']/df['calls_count']
df['avg_waittm'] = (df['total_call_duration'] - df['call_duration'])/df['calls_count']

df['wait_time'] = df['wait_time'].astype(int)
df['avg_waittm'] = df['avg_waittm'].astype(int)

# Преобразуем типы данных времени для подсчета
df['date'] = pd.to_datetime(df['date'],utc=False).dt.date
df['date_start'] = pd.to_datetime(df['date_start'],utc=False).dt.date
# Создадим столбец 
df['lvt_oper_id'] = df['date'] - df['date_start']
# Вернем тип данных в столбцах даты 
df['date'] = pd.to_datetime(df['date'])
df['date_start'] = pd.to_datetime(df['date_start'])

# Преобразуем период в число формата int, так необходимо для дальнейшего анализа
df['lvt_oper_id'] = df['lvt_oper_id'].dt.days
#df.info()
#df.head()

<a id='4'></a>

<font color='crimson'> 
<h2 align="center"> Исследовательский анализ данных</h2>
</font>
 

<a id='4-1'></a>

<font color='crimson'>

### Разделим колл-центры по специализации

на те, которые специализируются на обзвонах, и те, что специализируются на обработке входящих звонков.<br>
</font>

[К началу раздела](#4)<br>
[К соддержанию](#0)

Создадим новый столбец где будет обозначено к какой специализации относится колл-центр

Будем считать, что те колл-центры у которых **исходящих внешних** звонков больше, чем внешних входящих - специализируются на обзвонах.
И, напротив, те у кого входящих внешних больше - специализируются на обработке входящих звонков.

Сгруппируем данные

# выберем только внешние звонки
user_type = df.loc[df['internal'] == False]
# сгруппируем данные по клиентам
user_type = user_type.groupby(['user_id','direction'])['operator_id'].agg('count').reset_index()
# нам не нужен именно столбец operator_id - для подсчета count, так как пропусков нет - 
# нужен был любой столбец кроме группируемых, поэтому переименуем в count
user_type = user_type.rename(columns={'operator_id':'count'})

display(user_type)

print(
    'проверим не пропал ли кто из клиентов, \n должно быть', df['user_id'].nunique(),' клиентов, \n текущих клиентов:',
    user_type['user_id'].nunique()
     )

Теперь разделим колл-центры:

user_type = user_type.pivot_table(index='user_id', columns = 'direction').reset_index()
user_type = user_type.droplevel(0, axis=1)
user_type.columns = ['user_id','in','out']

user_type

user_type = user_type.fillna(0)
user_type['type'] = 0

for i in range(0,290):
    if user_type['in'][i] > user_type['out'][i]:
        user_type['type'][i] = 'прием'
    elif user_type['in'][i] < user_type['out'][i]:
        user_type['type'][i] = 'обзвон'
    else: user_type['type'][i] = 'неопределенный'

user_type

2 колл-центров по такой методике определить не удалось, посмотрим на них:

user_type.loc[user_type['type'] == 'неопределенный']

#user_type_graf = user_type[['тип_звонка', 'сумма_звонков']]
user_type_graf = user_type.groupby('type')['user_id'].count().reset_index()
user_type_graf = user_type_graf.sort_values(by='user_id', ascending=False)
user_type_graf
#call_type_group['процент, %'] = round((call_type_group['сумма_звонков']/call_type_group['сумма_звонков'].sum()),3)*100
#display(call_type_group)

Визуализируем данные:

plt.figure(figsize=(15,3))
sns.barplot(data=user_type_graf, y='type', x="user_id",palette=sns.color_palette("BuGn_r", n_colors=len(user_type_graf)))
plt.title('Распеределение по специализации колл-центров')
plt.xlabel('количество колл-центров')
plt.ylabel('специализация')
plt.show()

Объединим датафрейм с основным

user_type = user_type[['user_id', 'type']]

df2 = df.merge(user_type, on='user_id', how='left')
df2.head()

**Соберем колл-центры, которые специализируются на обработке входящих звонков<br>
и тех, которые специализируются на обзвоне**

# Соберем колл-центры, которые специализируются на обработке входящих звонков
df_in = df2.loc[df2['type'] == 'прием']
# Соберем колл-центры, которые специализируются на обзвоне
df_out = df2.loc[df2['type'] == 'обзвон']

print('Уникальных клиентов в колл-центрах,\n которые специализируются на обработке входящих звонков: ',df_in['user_id'].nunique(),'\n')
print(df_in.info())

print('Уникальных клиентов в колл-центрах, \n которые специализируются на обзвоне: ',df_out['user_id'].nunique(),'\n')
print(df_out.info())

<a id='4-2'></a>

<font color='crimson'>

### Проверим связь специализации с тарифным планом
</font>

[К началу раздела](#4)<br>
[К соддержанию](#0)

# Количество звонков на разных тарифах у разных клиентов
tariff_associated = df2.pivot_table(
    index='type', columns=('tariff_plan'), values='user_id', aggfunc='count').fillna(0)
tariff_associated

tariff_associated = df2.groupby(['type','tariff_plan'])['user_id'].nunique().reset_index()
tariff_associated['процент'] = round((tariff_associated['user_id']/tariff_associated['user_id'].sum()),3)*100
#tariff_associated['процент_от_типа'] = round((tariff_associated['user_id']/(delit)),3)*100
tariff_associated

tariff_associated = df_in.groupby(['type','tariff_plan'])['user_id'].nunique().reset_index()
tariff_associated['процент'] = round((tariff_associated['user_id']/tariff_associated['user_id'].sum()),3)*100
#tariff_associated['процент_от_типа'] = round((tariff_associated['user_id']/(delit)),3)*100
tariff_associated

tariff_associated = df_out.groupby(['type','tariff_plan'])['user_id'].nunique().reset_index()
tariff_associated['процент'] = round((tariff_associated['user_id']/tariff_associated['user_id'].sum()),3)*100
#tariff_associated['процент_от_типа'] = round((tariff_associated['user_id']/(delit)),3)*100
tariff_associated


Наиболее предпочтительным тарифом у колл-центров специализирующихся на приеме звонков является тариф `С`.
Но, по большому счету, связи между тарифным планом и специализацией колл_центров не обнаружена - и у обзванивающих и у принимающих звонки - тарифы распределены по разным категориям, без сильных перекосов.

<a id='4-3'></a>

<font color='crimson'>

### Оперелим границы эффективности операторов в колл-центрах,<br>которые **специализируются на обработке входящих звонков**
</font>

[К началу раздела](#4)<br>
[К соддержанию](#0)

<a id='4-3-1'></a>

<font color='crimson'>

#### по пропущенным внешним вызовам
</font>


Для начала отфильтруем только внешние входящие пропущенные:

missed_call = df_in.loc[(df_in['direction']=='in')&(df_in['internal']==False)&(df_in['is_missed_call']==True)]

Тепереь посмортим как часто операторы пропускали звонки в разрезе за каждый день:

missed_call = missed_call.groupby(['user_id','operator_id', 'lvt_oper_id'])['calls_count'].sum().reset_index()
# Переименуем целевой столбец чтобы отличать его от одноименного
missed_call = missed_call.rename(columns={'calls_count':'count_missed'})
# Отсортируем по убыванию пропущеных звонков
missed_call = missed_call.sort_values(by='count_missed', ascending=False)
missed_call

Наибольшее количество пропущенных внешних звонков - 13.
и далеее стрмительно уменьшается.
Напомню, что это в тех колл-центрах, которые специализируются на обработке входящих звонков.
Теперь определим 95 и 99 процентиль:

missed_call['count_missed'].quantile([0.95,0.99])

# Код ревьюера
df_in.loc[(df_in['direction']=='in')&(df_in['internal']==False)&(df_in['is_missed_call']==True)].head()

eff_missed_call = (missed_call['count_missed'].quantile(0.99)).astype(int)
eff_missed_call

<div style="border-radius: 22px; box-shadow: 4px 4px 4px; border: solid green 2px; padding: 16px">

**Вывод:**

Примем за неэффективных тех операторов, кто допустил больше 5 пропущенных вызовов.


<a id='4-3-2'></a>

<font color='crimson'>

#### по длительности ожидания ответа
</font>


avg_wait_call = df_in.groupby(['user_id','operator_id','date','lvt_oper_id'])['avg_waittm'].mean().reset_index()
avg_wait_call = avg_wait_call.sort_values(by='avg_waittm', ascending=False)
avg_wait_call.head(10)

 И снова самыми неэффективными оказались те, чьи ID отсутсвует.
 Отфильтруем по operator_id и сразу возьмем последний месяц исследуемого периода:

eff_wait_time = avg_wait_call['avg_waittm'].quantile([0.95,0.99])
eff_wait_time

до 39 секунд в среднем на звонок - допускают ожидание при ответе 95 % операторов, тех колл-центров, которые специализируются на обработке входящих звонков!
Кажется это много?
Но разве Вам не попадались эта фраза: "Ваш звонок очень важен для Вас, Вы 150 в очереди, Вам ответит ближайший проснувшийся оператор" ? После чего мы слушаем музыку.

eff_wait_time = (avg_wait_call['avg_waittm'].quantile(0.95)).astype(int)

<div style="border-radius: 22px; box-shadow: 4px 4px 4px; border: solid green 2px; padding: 16px">

**Вывод:**

Границей эффективности по ожиданию ответа на входящий внешний звонок будем считать по 95 процентилю - 39 секунд.

<a id='4-3-3'></a>

<font color='crimson'>

#### по длительности самого ответа
</font>


Сгруппируем данные - для каждого оператора в качестве среднего возьмем не только среднее время одного звонка, но общее время звонка, чтобы понимать сколько по времени оператору звонили.

# dur_resp образованно от duration_response
dur_resp = (df_in.groupby(['user_id',
                          'operator_id',
                          'date',
                          'lvt_oper_id'])[['avg_callt',
                                           'total_call_duration',
                                           'calls_count']].agg({'avg_callt':'mean',
                                                                'total_call_duration':'mean',
                                                                'calls_count':'sum'}).reset_index()
           )
dur_resp = dur_resp.sort_values(by='avg_callt', ascending=False)
dur_resp

Максимальная длительность ответа составила около 30 минут.
Но есть "сверхбыстрые" операторы, средний ответ которых составлял менее секунды.

Не могу не заметить количество принятых звонков в день: 1-2 - они янво не любители перерабатывать.
Уволить всех!

dur_resp[['avg_callt','total_call_duration','calls_count']].quantile([0.05,0.1,0.95,0.99])

plt.figure(figsize=(20,1))
plt.title('Графики межквартильного размаха',  fontsize=14)
sns.boxplot(data=dur_resp, x='avg_callt', medianprops={"color": "coral"})
plt.xlabel('Длительность ответа, сек', fontsize=12)
plt.xlim(250,0)
plt.show()
plt.figure(figsize=(20,1))
sns.boxplot(data=dur_resp, x='total_call_duration', medianprops={"color": "coral"})
plt.xlabel('Общая длительность звонка с учетом ожидания на ответ, сек', fontsize=12)
plt.xlim(2000,0)
plt.show()
plt.figure(figsize=(20,1))
sns.boxplot(data=dur_resp, x='calls_count', medianprops={"color": "coral"})
plt.xlabel('Количество звонков, шт', fontsize=12)
plt.xlim(50,0)
plt.show()

Считаю, что верхнюю границу по длительности ответа брать по 95 процентилю - не стоит, так как это меньше 5 минут. Лучше возьмем по 99 процентилю - если ты регулярно тратитшь на ответ дольше, чем 99 % операторов - ты либо слишком долгий, либо надо проанализировать, что в твоей работе не так.

Нижняя граница в 8 секунд на мой взгялд вполне адекватна - если мы рассматриваем колл-центры, которые специализируются на обработке входящих звонков, то входящие звонки идут от людей с какими-то вопросами.
За 8 секунд можно выслушать вопрос, и положить трубку.

lower_eff_resp = (dur_resp['avg_callt'].quantile(0.05)).astype(int)
upper_eff_resp = (dur_resp['avg_callt'].quantile(0.99)).astype(int)
print(lower_eff_resp,'\n', upper_eff_resp)

<div style="border-radius: 22px; box-shadow: 4px 4px 4px; border: solid green 2px; padding: 16px">

**Вывод:**

Принимаем:
    За нижнюю границу эффективности по длительности ответа = 8 секунд,
    за верхнюю = 571 секунда.

print(f'eff_missed_call - граница по количеству пропущенных внешних входящих в месяц = {round(eff_missed_call,0)} шт.')
print(f'eff_wait_time - граница по среднему времени ожидания = {round(eff_wait_time,0)} секунд')
print(f'lower_eff_resp - нижняя граница по среднему времени ответа = {round(lower_eff_resp,0)} секунд')
print(f'upper_eff_resp - верхняя граница по среднему времени ответа = {round(upper_eff_resp,0)} секунд')


<a id='4-4'></a>

<font color='crimson'>

### Опереление границ эффективности операторов в колл-центрах,<br>которые **специализируются на обзвонах**
</font>

[К началу раздела](#4)<br>
[К соддержанию](#0)

<a id='4-4-1'></a>

<font color='crimson'>

#### по количеству исходящих внешних звонков
</font>


# dur_resp образованно от duration_response
# ОТсортируем только исходящие внещние
dur_resp_out = df_out.query('direction == "out" and internal ==False')

dur_resp_out = dur_resp_out.groupby(['user_id','operator_id'])['calls_count'].sum().reset_index()
# Отсортируем по убыванию пропущеных звонков
dur_resp_out = dur_resp_out.sort_values(by='calls_count', ascending=False)
dur_resp_out

dur_resp_out['calls_count'].quantile([0.05,0.1,0.95,0.99])

out_lower_eff = (dur_resp_out['calls_count'].quantile(0.05)).astype(int)
out_upper_eff = (dur_resp_out['calls_count'].quantile(0.95)).astype(int)

print('Сводные данные по границам эффективности:')
print(f'    eff_missed_call - граница по количеству пропущенных внешних входящих в месяц = {round(eff_missed_call,0)} шт.')
print(f'    eff_wait_time - граница по среднему времени ожидания = {round(eff_wait_time,0)} секунд')
print(f'    lower_eff_resp - нижняя граница по среднему времени ответа = {round(lower_eff_resp,0)} секунд')
print(f'    upper_eff_resp - верхняя граница по среднему времени ответа = {round(upper_eff_resp,0)} секунд')
print(f'    out_lower_eff - нижняя граница по количеству исходящих внешних звонков = {out_lower_eff} шт.')
print(f'    out_upper_eff - верхняя граница по количеству исходящих внешних звонков = {out_upper_eff} шт.')


<a id='4-4-2'></a>

<font color='crimson'>

#### по продолжительности самого звонка
</font>


df_out.head()

wait_call = df_out.groupby(['user_id','operator_id','date'])['avg_callt'].mean().reset_index()
wait_call = wait_call.sort_values(by='avg_callt', ascending=False)
wait_call.head(10)

wait_call['avg_callt'].quantile([0.05,0.1,0.95,0.99]).reset_index()

out_lower_call = (wait_call['avg_callt'].quantile(0.10)).astype(int)
out_upper_call = (wait_call['avg_callt'].quantile(0.99)).astype(int)

print('Сводные данные по границам эффективности:')
print(' для колл-центров, которые специализируются на обработке входящих звонков:')
print(f'    eff_missed_call - граница по количеству пропущенных внешних входящих = {eff_missed_call} шт.')
print(f'    eff_wait_time - граница по среднему времени ожидания = {eff_wait_time} секунд')
print(f'    lower_eff_resp - нижняя граница по среднему времени ответа = {round(lower_eff_resp,0)} секунд')
print(f'    upper_eff_resp - верхняя граница по среднему времени ответа = {round(upper_eff_resp,0)} секунд')
print(' для колл-центров, которые специализируются на обзвонах:')
print(f'    out_lower_eff - нижняя граница по количеству исходящих внешних звонков = {round(out_lower_eff,0)} шт.')
print(f'    out_upper_eff - верхняя граница по количеству исходящих внешних звонков = {round(out_upper_eff,0)} шт.')
print(f'    out_lower_call - нижняя граница по средней длительности общения исходящего внешнего звонка = {round(out_lower_call,0)} секунд')
print(f'    out_upper_call - верхняя граница по средней длительности общения исходящего внешнего звонка = {round(out_upper_call,0)} секунд')


<a id='5'></a>

<font color='crimson'>
<h2 align="center"> Определим неэффективных операторов</h2>
</font>

<a id='5-1'></a>

<font color='crimson'>

### Определим неэффективных операторов в колл-центрах, <br>которые **специализируются на обработке входящих звонков**

</font>

[К началу раздела](#5)<br>
[К соддержанию](#0)

<a id='5-1-1'></a>

<font color='crimson'>

#### по количеству пропущенных внешних вызовов 
</font>


# Найдем кумулятивную сумму звонков на каждого опреатора за каждый день
mse = missed_call.groupby(['operator_id']).sum() \
  .groupby(level=0).cumsum().reset_index().sort_values(by=['count_missed'], ascending=False)
mse.head()

<div style="border-radius: 22px; box-shadow: 4px 4px 4px; border: solid green 2px; padding: 16px">

**Выведем операторов кто совершал пропущенных звонков больше границы эффективности:**

print(f'    eff_missed_call - граница по количеству пропущенных внешних входящих = {eff_missed_call} шт.')

ineff_missed_call = mse.loc[mse['count_missed'] > eff_missed_call, ['user_id', 'operator_id', 'count_missed']]
ineff_missed_call

Найдем этих операторов в основном датасете:

list_ineff_missed_call = ineff_missed_call['operator_id']

list_ineff_missed_call = df_in.query('operator_id in @list_ineff_missed_call').sort_values(by=['date'], ascending=False)
list_ineff_missed_call

# Сгруппируем 
ineff_missed_call_graf = list_ineff_missed_call.groupby(['user_id', 'operator_id'])['lvt_oper_id'].first().reset_index()
ineff_missed_call_graf

ineff_missed_call = ineff_missed_call[['operator_id', 'count_missed']]
ineff_missed_call_graf = ineff_missed_call_graf.merge(ineff_missed_call, on='operator_id', how='left')
ineff_missed_call_graf

ineff_missed_call_graf['пропущенные_относительно_лайфтайма'] = round(((ineff_missed_call_graf['count_missed'] / ineff_missed_call_graf['lvt_oper_id'])*100),1)
ineff_missed_call_graf.sort_values(by=['пропущенные_относительно_лайфтайма'], ascending=False)


list_mse = mse['operator_id']

list_mse = df_in.query('operator_id in @list_mse').sort_values(by=['date'], ascending=False)
list_mse.head()

# Сгруппируем 
list_mse_graf = list_mse.groupby(['user_id', 'operator_id'])['lvt_oper_id'].max().reset_index()
list_mse_graf.head(10)

mse = mse[['operator_id', 'count_missed']]
list_mse_graf = list_mse_graf.merge(mse, on='operator_id', how='left')
list_mse_graf.head()

list_mse_graf['процент_пропущенных_относительно_лайфтайма'] = round(((list_mse_graf['count_missed'] / list_mse_graf['lvt_oper_id'])*100),1)
list_mse_graf = list_mse_graf.sort_values(by=['процент_пропущенных_относительно_лайфтайма'], ascending=False)
list_mse_graf.head(10)

Топ 10 с самой высокой долей внешних пропущенных вызовов

list_mse_graf = list_mse_graf.head(10)
list_mse_graf = list_mse_graf[['процент_пропущенных_относительно_лайфтайма', 'operator_id']]
list_mse_graf

plt.figure(figsize=(15,5))
sns.barplot(data=list_mse_graf, y='процент_пропущенных_относительно_лайфтайма', 
            x="operator_id", 
            palette=sns.color_palette("BuGn_r", n_colors=len(list_mse_graf)),
           order=list_mse_graf.groupby('operator_id')['процент_пропущенных_относительно_лайфтайма'].agg('mean').sort_values(ascending=False).index)
plt.title('Топ 10 операторов с самой высокой долей внешних пропущенных вызовов')
plt.xlabel('operator_id')
plt.ylabel('процент_пропущенных_относительно_лайфтайма')
plt.show()

<a id='5-1-2'></a>

<font color='crimson'>

#### по длительности ожидания ответа

</font>


print(f'    eff_wait_time - граница по среднему времени ожидания = {eff_wait_time} секунд')

# Найдем среднее время ожидания вызова для каждого опеартора
ineff_wait_time = avg_wait_call.groupby('operator_id').mean().reset_index()
# Отсортируем их по границе эффективности
ineff_wait_time = ineff_wait_time.query('avg_waittm > @eff_wait_time')
ineff_wait_time=ineff_wait_time[['user_id', 'operator_id', 'avg_waittm']]
ineff_wait_time 

Для арезентации оставим только 10 самых самых:

ineff_wait_time_graf = ineff_wait_time.sort_values(by=['avg_waittm'], ascending=False).head(10)

plt.figure(figsize=(16,5))
sns.barplot(data=ineff_wait_time_graf, y='avg_waittm', 
            x="operator_id", 
            palette=sns.color_palette("BuGn_r", n_colors=len(ineff_wait_time_graf)), 
            order=ineff_wait_time_graf.groupby('operator_id')['avg_waittm'].agg('mean').sort_values(ascending=False).index)
plt.title('Топ 10 операторов с самой большой средней \n продолжительностью ожидания ответа на внешний входящий звонок')
plt.xlabel('operator_id')
plt.ylabel('время ожидания ответа \n на внешний входящий звонок, сек')
plt.show()

<a id='5-1-3'></a>

<font color='crimson'>

#### по длительности ответа
</font>


print(f'    lower_eff_resp - нижняя граница по среднему времени ответа = {round(lower_eff_resp,0)} секунд')
# Найдем среднее время ответа для каждого опеартора
ineff_dur_resp  = dur_resp.groupby('operator_id').mean()
# Отсортируем их по нижней границе эффективности
ineff_lower_resp = ineff_dur_resp.query('avg_callt < @lower_eff_resp').reset_index() 
ineff_lower_resp = ineff_lower_resp[['user_id', 'operator_id', 'avg_callt']]
ineff_lower_resp

ineff_lower_resp = ineff_lower_resp.sort_values(by=['avg_callt'], ascending=False)

plt.figure(figsize=(7,4))
sns.barplot(data=ineff_lower_resp, y='avg_callt', 
            x="operator_id", 
            palette=sns.color_palette("BuGn_r", n_colors=len(ineff_lower_resp)), 
            order=ineff_lower_resp.groupby('operator_id')['avg_callt'].agg('mean').sort_values(ascending=False).index)
plt.title('Операторы, средняя длительность ответа котроых \n НИЖЕ границы эффективности')
plt.xlabel('operator_id')
plt.ylabel('средняя длительность ответа \n на внешний входящий звонок, сек')
plt.show()

print(f'    upper_eff_resp - верхняя граница по среднему времени ответа = {round(upper_eff_resp,0)} секунд')

# Отсортируем их по верхней границе эффективности
ineff_upper_resp = ineff_dur_resp.query('avg_callt > @upper_eff_resp').reset_index() 
ineff_upper_resp = ineff_upper_resp[['user_id', 'operator_id', 'avg_callt']].sort_values(by=['avg_callt'], ascending=False)
ineff_upper_resp 

plt.figure(figsize=(7,4))
sns.barplot(data=ineff_upper_resp, y='avg_callt', 
            x="operator_id", 
            palette=sns.color_palette("BuGn_r", n_colors=len(ineff_upper_resp)), 
            order=ineff_upper_resp.groupby('operator_id')['avg_callt'].agg('mean').sort_values(ascending=False).index)
plt.title('Операторы, средняя длительность ответа котроых \n ВЫШЕ границы эффективности')
plt.xlabel('operator_id')
plt.ylabel('средняя длительность ответа \n на внешний входящий звонок, сек')
plt.show()

<a id='5-2'></a>

<font color='crimson'>

### Определим неэффективных операторов в колл-центрах,<br> которые **специализируются на обзвонах**

</font>

[К началу раздела](#5)<br>
[К соддержанию](#0)

<a id='5-2-1'></a>

<font color='crimson'>

#### по количеству исходящих внешних звонков
</font>


# Найдем кумулятивную сумму звонков на каждого опреатора за каждый день
ish = dur_resp_out.groupby(['operator_id']).sum() \
  .groupby(level=0).cumsum().reset_index().sort_values(by=['calls_count'], ascending=False)
ish.head()

print(f'    out_lower_eff - нижняя граница по количеству исходящих внешних звонков = {round(out_lower_eff,0)} шт.')

ineff_out_lower_resp = ish.loc[ish['calls_count'] < out_lower_eff, ['user_id', 'operator_id', 'calls_count']]
ineff_out_lower_resp

print(f'    out_upper_eff - верхняя граница по количеству исходящих внешних звонков = {round(out_upper_eff,0)} шт.')

ineff_out_dur_resp = ish.loc[ish['calls_count'] > out_upper_eff, ['user_id', 'operator_id', 'calls_count']]
ineff_out_dur_resp

Меньше одного исходящего не совершал ни один оператор.
Посмотрим на всем датасете:

list_ish = ish['operator_id']

list_ish = df_out.query('operator_id in @list_ish').sort_values(by=['date'], ascending=False)
list_ish.head()

# Сгруппируем 
list_ish_graf = list_ish.groupby(['user_id', 'operator_id'])['lvt_oper_id'].max().reset_index()
list_ish_graf.head(10)

# Итоговая таблица
ish = ish[['operator_id', 'calls_count']]
list_ish_graf = list_ish_graf.merge(ish, on='operator_id', how='left')
list_ish_graf = list_ish_graf.query('lvt_oper_id > 0')
list_ish_graf.head()

list_ish_graf['исходящих_внешних_средн_в_день'] = round(((list_ish_graf['calls_count'] / list_ish_graf['lvt_oper_id'])),0)
list_ish_graf = list_ish_graf.sort_values(by=['исходящих_внешних_средн_в_день'], ascending=False)
list_ish_graf.head(10)

Топ 10 операторов с самой высокой частотой исходящих внешних в среднем в день 

list_ish_graf = list_ish_graf.head(10)
list_ish_graf = list_ish_graf[['исходящих_внешних_средн_в_день', 'operator_id']]
list_ish_graf

plt.figure(figsize=(15,5))
sns.barplot(data=list_ish_graf, y='исходящих_внешних_средн_в_день', 
            x="operator_id", 
            palette=sns.color_palette("BuGn_r", n_colors=len(list_ish_graf)),
           order=list_ish_graf.groupby('operator_id')['исходящих_внешних_средн_в_день'].agg('mean').sort_values(ascending=False).index)
plt.title('Топ 10 операторов с самой высокой средней частотой исходящих внешних звонков в день')
plt.xlabel('operator_id')
plt.ylabel('средняя частота внешних \nисходящих звонков в день, шт.')
plt.show()

<a id='5-2-2'></a>

<font color='crimson'>

#### по продолжительности звонка
</font>


# Найдем среднее время длительности одного звонка для каждого опеартора
ineff_out_dur_resp  = wait_call.groupby('operator_id').mean()

print(f'    out_lower_call - нижняя граница по средней длительности общения исходящего внешнего звонка = {round(out_lower_call,0)} секунд')

# Отсортируем их по нижней границе эффективности
ineff_out_lower_call = ineff_out_dur_resp.query('avg_callt < @out_lower_call').reset_index() 
ineff_out_lower_call = ineff_out_lower_call[['user_id', 'operator_id', 'avg_callt']]
ineff_out_lower_call = ineff_out_lower_call.sort_values(by='avg_callt', ascending=False).head(10)
ineff_out_lower_call

plt.figure(figsize=(15,5))
sns.barplot(data=ineff_out_lower_call, y='avg_callt', 
            x="operator_id", 
            palette=sns.color_palette("BuGn_r", n_colors=len(ineff_out_lower_call)),
           order=ineff_out_lower_call.groupby('operator_id')['avg_callt'].agg('mean').sort_values(ascending=False).index)
plt.title('Топ 10 операторов, средняя продолжительность звонка котроых \n НИЖЕ границы эффективности')
plt.xlabel('operator_id')
plt.ylabel('средняя продолжительность \n исходящих внешнего звонка, сек')
plt.show()

print(f'    out_upper_call - верхняя граница по средней длительности общения исходящего внешнего звонка = {round(out_upper_call,0)} секунд')
# Отсортируем их по верхней границе эффективности
ineff_out_upper_call = ineff_out_dur_resp.query('avg_callt > @out_upper_call').reset_index() 
ineff_out_upper_call = ineff_out_upper_call[['user_id', 'operator_id', 'avg_callt']]
ineff_out_upper_call 

plt.figure(figsize=(5,5))
sns.barplot(data=ineff_out_upper_call, y='avg_callt', 
            x="operator_id", 
            palette=sns.color_palette("BuGn_r", n_colors=len(ineff_out_upper_call)),
           order=ineff_out_upper_call.groupby('operator_id')['avg_callt'].agg('mean').sort_values(ascending=False).index)
plt.title('Операторы, средняя продолжительность звонка котроых \n ВЫШЕ границы эффективности')
plt.xlabel('operator_id')
plt.ylabel('средняя продолжительность \n исходящих внешнего звонка, сек')
plt.show()

<a id='6'></a>

<font color='crimson'>
<h2 align="center"> Проверим гипотезы</h2>
    
</font>

<a id='6-1'></a>

<font color='crimson'>

### Эффективность работы оперторов в выходные дни и в будни отличается

</font>

[К началу раздела](#6)<br>
[К соддержанию](#0)

Эффективность работы оперторов будем оценивать по количеству внешних не пропущенных звонков (исходящих/входящих не важно). Главное, что это - внешние звонки, т.е. связаные с работой с клиентами, а не внутренние - связанные с решением внутренних вопросов. Так как звонков в принципе больше в будние дни чем в выходные, мы будем руководтсвовоаться относительными значениями - будем смтреть количество только внешних непропущенных звонков относительно всех звонков.

При проверке статистических гипотез будем использовать t-критерий Стьюдента, так как количество наблюдений достаточно много и в таком случае уже не важно будет ли распрделение нормальным. Так же в наших выборках есть совпадающие значения. 

**Сформулируем гипотезы:** 

**H_0**: Относительное количество внешних (internal==False) непропущенных(is_missed_call==False) звонков (calls_count) в **будние** дни<br> ***равно*** <br>относительному количеству внешних (internal==False) <br>непропущенных(is_missed_call==False) <br>звонков (calls_count) в **выходные** дни.<br>
**H_1**: Относительное количество внешних (internal==False) непропущенных(is_missed_call==False) звонков (calls_count) в **будние** дни <br>***отличается*** от<br> относительного количества внешних (internal==False)непропущенных(is_missed_call==False) звонков (calls_count) в **выходные** дни.

Значение уровня значимости зададим
`alpha` = 0.05


Для начала создадим вспомогательный столбец "дни недели":

df2['weekday'] = df2['date'].dt.day_name()
df2['weekday'].unique()

Сделаем выборки:

weekdays = df2.query('weekday != "Sunday" and  weekday != "Saturday" and internal==False and is_missed_call==False')['calls_count']
weekdays_total = df2.query('weekday != "Sunday" and  weekday != "Saturday"')['calls_count']

weekend = df2.query('weekday == "Sunday" or  weekday == "Saturday" and internal==False and is_missed_call==False')['calls_count']
weekend_total = df2.query('weekday == "Sunday" or  weekday == "Saturday"')['calls_count']

# Процент пропущенных в будние дни
perc_weekdays = (weekdays/weekdays_total).fillna(0)
print(perc_weekdays.head(), len(perc_weekdays))
# Процент пропущенных в выходные дни
perc_weekend = (weekend/weekend_total).fillna(0)
print(perc_weekend.head(), len(perc_weekend))

# Зададим значение уровня значимости alpha
alpha = 0.05

# Вызовем метод для проверки гипотезы
results = scipy.stats.ttest_ind(perc_weekdays, perc_weekend, equal_var=False)

# вывод значения p-value на экран
print('p-значение =', results.pvalue)

if (results.pvalue < alpha):
     print(
        'p-значение меньше уровня значимости, значит \n'+ 
        'нулевую гипотезу о том, что относительное количество внешних звонков операторами \n'+
        'в будни и в выходные дни одинаковое - отвергаем')
else:
    print('p-значение больше уровня значимости, значит \n'+ 
        'нулевую гипотезу о том, что относительное количество внешних звонков операторами \n'+
        'в будни и в выходные дни одинаковое - отвергнуть не получится')


<a id='6-2'></a>

<font color='crimson'>

### В выходные дни и в будни относительное количество пропущенных звонков одинаковое

</font>

[К началу раздела](#6)<br>
[К соддержанию](#0)

Пропущенные звонки будем считать относительно всех звонков в этот день.
Будем как внешние тик и внутренние пропущенные.

При проверке статистических гипотез так же будем использовать t-критерий Стьюдента, так как количество наблюдений достаточно много и в таком случае уже не важно будет ли распрделение нормальным. Так же в наших выборках есть совпадающие значения. 

**Сформулируем гипотезы:** 

**H_0**: Относительное количество пропущенных(is_missed_call==True) звонков (calls_count) в **будние** дни<br> ***равно*** <br>относительному количеству пропущенных(is_missed_call==True) звонков (calls_count) в **выходные** дни.<br>
<br>
**H_1**: Относительное количество пропущенных(is_missed_call==True) звонков (calls_count) в **будние** дни <br>***отличается*** от<br> относительного количества пропущенных(is_missed_call==True) звонков (calls_count) в **выходные** дни.

Значение уровня значимости зададим
`alpha` = 0.05


# Найдем количество пропущенных в будние дни
perc_weekdays_delimoe = df2.query('weekday != "Sunday" and  weekday != "Saturday" and is_missed_call==True')['calls_count']

# Найдем количество пропущенных в выходные дни
perc_weekend_delimoe = df2.query('weekday == "Sunday" or  weekday == "Saturday" and is_missed_call==True')['calls_count']

# Процент пропущенных в будние дни
perc_weekdays = (perc_weekdays_delimoe/weekdays_total).fillna(0)
print(perc_weekdays.head(), len(perc_weekdays))
# Процент пропущенных в выходные дни
perc_weekend = (perc_weekend_delimoe/weekend_total).fillna(0)
print(perc_weekend.head(), len(perc_weekend))

perc_weekend.unique()

alpha = 0.05
results = scipy.stats.ttest_ind(perc_weekdays, perc_weekend, equal_var=False)

print('p-значение =', results.pvalue)

if (results.pvalue < alpha):
    print(
        'p-значение меньше уровня значимости, значит \n'+ 
        'нулевую гипотезу о том, что относительное количество пропущенных звонков операторами \n'+
        'в будни и в выходные дни одинаковое - отвергаем')
else:
    print('p-значение больше уровня значимости, значит \n'+ 
        'нулевую гипотезу о том, что относительное количество пропущенных звонков операторами \n'+
        'в будни и в выходные дни одинаковое - отвергнуть не получится')

<div style="border-radius: 22px; box-shadow: 4px 4px 4px; border: solid green 2px; padding: 16px">

**Выводы:**
    
Мы проверили две гипотезы статистичесчким методом.

<a id='7'></a>

<font color='crimson'>
<h2 align="center"> Выводы</h2>
    
</font>

В данном проекте
* Мы выполнили предобработку:
    * Объединили оба датафрейма, для удобства анализа тарифов и опыта операторов,
    * Удалили дубликаты, так как данные агрегированные и дублей по агрегационным признакам быть не должно,
    * Обработали пропуски в следующих столбцах:
        * `operator_id` - выяснили, что пропуски могли возникнуть из-за того, что программа в некоторых случаях не успела назначить оператора для ответа, либо звонок был в нерабочее время, либо отвечат автоответчик,
        * `internal` - природу возникновения пропусков выяснить не удалось - пропуски удалили,
    * Обнаружили несоответствие данных в столбце `is_missed_call`, которое привели в порядок,
    * Преобразовали типы данных,
    * Убрали выбросы,
    * Проверили на аномалии категориальные столбцы,
    * Проверили корреляцию,
    * Создали несколько вспомогательных столбцов, необходимых для анализа.
* Выполнили исследовательский анализ, где
    * Разделили колл-центры по специализации на те, которые специализируются на обработке входящих звонков и те, которые специализируются на обзвонах,
    * Проверили связь специализации колл-центров с тарифными планами - связь не обнаружили.
    * Оперделили границы эфеективности по различным признакам, отдельно дл колл-центров разной специализации:

print('Сводные данные по границам эффективности:')
print(' для колл-центров, которые специализируются на обработке входящих звонков:')
print(f'    eff_missed_call - граница по количеству пропущенных внешних входящих в месяц = {round(eff_missed_call,0)} шт.')
print(f'    eff_wait_time - граница по среднему времени ожидания = {round(eff_wait_time,0)} секунд')
print(f'    lower_eff_resp - нижняя граница по среднему времени ответа = {round(lower_eff_resp,0)} секунд')
print(f'    upper_eff_resp - верхняя граница по среднему времени ответа = {round(upper_eff_resp,0)} секунд')
print(' для колл-центров, которые специализируются на обзвонах:')
print(f'    out_lower_eff - нижняя граница по количеству исходящих внешних звонков = {round(out_lower_eff,0)} шт.')
print(f'    out_upper_eff - верхняя граница по количеству исходящих внешних звонков = {round(out_upper_eff,0)} шт.')
print(f'    out_lower_call - нижняя граница по средней длительности общения исходящего внешнего звонка = {round(out_lower_call,0)} секунд')
print(f'    out_upper_call - верхняя граница по средней длительности общения исходящего внешнего звонка = {round(out_upper_call,0)} секунд')


Далее мы нашли операторов по определенным выше границам эффективности, для соответсвующих колл-центров, которых оказалось не много.

После проверили статистические гипотезы:
* Отвергли гипотезу о том, что относительное количество внешних звонков операторами в будни и в выходные дни одинаковое,
* Отвергли гипотезу о том, что относительное количество пропущенных звонков операторами в будни и в выходные дни одинаковое.

**По разработаке методики, по которой можно будет определять неэффективных операторов автоматически, предлагаю определять неэффективных операторов отдельно исходя из специализации колл-центров и по каждому признаку отдельно.**

Благодарю за внимание!<br>
[В начало](#nachalo)

Сохранение нужного файла для дашборда

df2.to_csv('Telecom_final_base.csv')
